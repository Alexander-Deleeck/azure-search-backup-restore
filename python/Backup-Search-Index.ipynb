{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook contains information on how to extract content from an Azure Cognitive Search index\n",
    "\n",
    "If you have not already done so, please install the Auzre Cognitive Search Python SDK:\n",
    "    !pip install azure-search-documents\n",
    "\n",
    "Important - Please Read\n",
    "Search indexes are different from other datastores in that it is really hard to extract all content from the store. Due to the way that search indexes are constantly ranking and scoring results, paging through search results or even using continuation tokes as this tool does has the possibility of missing data during data extraction. As an example, lets say you search for all documents, and there is a document with ID 101 that is part of page 5 of the search results. As you start extracting data from page to page as you move from page 4 to page 5, it is possible that now ID 101 is actually now part of page 4, meaning that when you look at page 5, it is no longer there and you just missed that document.\n",
    "\n",
    "For that reason, this tool keeps a count of the ID's of the keys extracted and will do a comparison to the count of documents in the Azure Search index to make sure they match. Although this does not provide a perfect solution, it does help reduce the chance of missing data.\n",
    "\n",
    "Also, as an extra precaution, it is best if there are no changes being made and the search index is in a steady state during this extraction phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ComplexField,\n",
    "    CorsOptions,\n",
    "    SearchIndex,\n",
    "    ScoringProfile,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField\n",
    ")\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "\n",
    "import math\n",
    "import base64\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import threading\n",
    "\n",
    "import pickle\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sample uses version: 11.1.0b3\n",
    "!pip show azure-search-documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the service endpoint and API key from the environment\n",
    "\n",
    "# service_name = \"SEARCH_ENDPOINT\"\n",
    "# admin_key = \"SEARCH_API_KEY\"\n",
    "# index_name = \"SEARCH_INDEX_NAME_TO_BE_RESTORED\"\n",
    "\n",
    "# Set the location where data will be backed up - this will be deleted and re-created\n",
    "# output_dir = \"/datadrive2/search-backup\"\n",
    "\n",
    "# Set the facet field - this should be a facetable field where no one single value has more than\n",
    "# 100K documents associated with it\n",
    "# facet_field = \"FACET_FIELD\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = '2020-06-30'\n",
    "\n",
    "\n",
    "# Create an SDK client\n",
    "endpoint = \"https://{}.search.windows.net/\".format(service_name)\n",
    "admin_client = SearchIndexClient(endpoint=endpoint,\n",
    "                      index_name=index_name,\n",
    "                      credential=AzureKeyCredential(admin_key),\n",
    "                      api_version=api_version)\n",
    "\n",
    "search_client = SearchClient(endpoint=endpoint,\n",
    "                      index_name=index_name,\n",
    "                      credential=AzureKeyCredential(admin_key),\n",
    "                      api_version=api_version)\n",
    "\n",
    "valid_facet_types = ['Edm.String']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the output directory where data will be backed up\n",
    "output_path = Path(output_dir)\n",
    "if output_path.exists():\n",
    "    rmtree(output_path)\n",
    "\n",
    "output_path_data = Path(os.path.join(output_dir, 'data'))\n",
    "output_path_data.mkdir(parents=True)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all fields in the index\n",
    "index_schema = admin_client.get_index(index_name)\n",
    "fields = []\n",
    "for field in index_schema.fields:\n",
    "    fields.append(field.name)\n",
    "\n",
    "print ('Found Fields:', fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the chosen facet is a correct type    \n",
    "facet_values = {}\n",
    "facet_too_large = False\n",
    "large_facet_str = ''\n",
    "\n",
    "valid_facet = False\n",
    "for field in index_schema.fields:\n",
    "    if field.name == facet_field:\n",
    "        if field.type in valid_facet_types:\n",
    "            valid_facet = True\n",
    "            break\n",
    "            \n",
    "if valid_facet == False:\n",
    "    print ('Error: Please choose a facet field that is one of', valid_facet_types)\n",
    "else:\n",
    "    print ('Validated facet field is of correct type')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the schema to the output dir\n",
    "with open(os.path.join(output_dir, 'schema.pkl'), 'wb') as schema_out:\n",
    "    pickle.dump(index_schema, schema_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the possible data values for this facet\n",
    "results = search_client.search(search_text=\"*\", facets=[facet_field + \",count:0\"], top=0)\n",
    "\n",
    "facet_values = {}\n",
    "facet_too_large = False\n",
    "large_facet_str = ''\n",
    "\n",
    "for facet in results.get_facets()[facet_field]:\n",
    "    if facet['count'] > 100000:\n",
    "        facet_too_large = True\n",
    "        large_facet_str = '\"' + facet['value'] + '\" has ' + str(facet['count']) + ' documents'\n",
    "        break\n",
    "    facet_values[facet['value']] = facet['count']\n",
    "\n",
    "if facet_too_large == False:\n",
    "    print ('Found', len(facet_values), 'facet values')\n",
    "else:\n",
    "    print ('Error - Facet has to many documents:', large_facet_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Data to data output dir\n",
    "def process_facet(f):\n",
    "    estimated_docs_to_extract = facet_values[f]\n",
    "    total_pages = math.ceil(estimated_docs_to_extract / page_size)\n",
    "    print ('Extracting', estimated_docs_to_extract, 'values for facet:', f, 'with', total_pages, 'total page(s)...')\n",
    "    for page in range(total_pages):\n",
    "        results =  search_client.search(search_text=\"*\",filter=facet_field + \" eq '\" + f + \"'\", top=page_size)\n",
    "        jsonobj = []\n",
    "        for result in results:\n",
    "            del result['@search.score']\n",
    "            del result['@search.highlights']\n",
    "            jsonobj.append(result)\n",
    "            file_name = base64.urlsafe_b64encode(f.encode()).decode()\n",
    "            with open(os.path.join(output_path_data, file_name + '-' + str(page) + '.json'), 'w') as data_out:\n",
    "                json.dump(jsonobj, data_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_size = 1000\n",
    "\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "results = Parallel(n_jobs=num_processes, verbose=50)(delayed(\n",
    "    process_facet)(f) for f in facet_values)\n",
    "\n",
    "# for f in facet_values:\n",
    "#     process_facet(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the extracted content and validate they can be loaded, as well as to get counts\n",
    "files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(os.path.join(output_dir, 'data')) for f in filenames]\n",
    "data_files = []\n",
    "error_files = []\n",
    "for file in files:\n",
    "    if 'error-' in file:\n",
    "        error_files.append(file)\n",
    "    else:\n",
    "        data_files.append(file)\n",
    "\n",
    "# Get count of data extracted\n",
    "doc_counter = 0\n",
    "error_counter = len(error_files)\n",
    "\n",
    "for file in data_files:\n",
    "    with open(file, \"r\") as f_in:\n",
    "        data = json.loads(f_in.read())\n",
    "        doc_counter += len(data)\n",
    "\n",
    "print ('Total Documents Exported:', doc_counter)\n",
    "print ('Total Documents Failed to Export:', error_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
